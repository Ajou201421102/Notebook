{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "masked_multi_head_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ajou201421102/Notebook/blob/main/masked_multi_head_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KsBGZpKkWki"
      },
      "source": [
        "##**8. Masked Multi-head Attention**\r\n",
        "1. Masked Multi-head Attention 구현.\r\n",
        "2. Encoder-Decoder Attention 구현."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRU5DFY2OM8"
      },
      "source": [
        "### **필요 패키지 import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDtMioSQQ1bB"
      },
      "source": [
        "from torch import nn\r\n",
        "from torch.nn import functional as F\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "import torch\r\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBiZObgRep_Q"
      },
      "source": [
        "### **데이터 전처리**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfTSaGYteuze"
      },
      "source": [
        "데이터의 값과 형태를 좀 더 명확하게 보기 위해 sample을 줄이겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ULZIqTenSc"
      },
      "source": [
        "pad_id = 0\r\n",
        "vocab_size = 100\r\n",
        "\r\n",
        "data = [\r\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13],\r\n",
        "  [60, 96, 51, 32, 90],\r\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21],\r\n",
        "  [66, 88, 98, 47],\r\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23]\r\n",
        "]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hx3mcivgMyH"
      },
      "source": [
        "def padding(data):\r\n",
        "  max_len = len(max(data, key=len))\r\n",
        "  print(f\"Maximum sequence length: {max_len}\")\r\n",
        "\r\n",
        "  for i, seq in enumerate(tqdm(data)):\r\n",
        "    if len(seq) < max_len:\r\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\r\n",
        "\r\n",
        "  return data, max_len"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3e8FiNvgX60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22bb19a6-01d5-407e-a62f-54f2f178ab05"
      },
      "source": [
        "data, max_len = padding(data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 12595.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Maximum sequence length: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwPSIWYugaN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a7bd55-7c78-463c-d61e-8b277565a47f"
      },
      "source": [
        "data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[62, 13, 47, 39, 78, 33, 56, 13, 0, 0],\n",
              " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0],\n",
              " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21],\n",
              " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0],\n",
              " [77, 65, 51, 77, 19, 15, 35, 19, 23, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwqjACx8iidc"
      },
      "source": [
        "### **Hyperparameter 세팅 및 embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Ngp2nWimS8"
      },
      "source": [
        "d_model = 8  # model의 hidden size\r\n",
        "num_heads = 2  # head의 개수\r\n",
        "inf = 1e12"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJMi2Xsni5uq"
      },
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\r\n",
        "\r\n",
        "# B: batch size, L: maximum sequence length\r\n",
        "batch = torch.LongTensor(data)  # (B, L)\r\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tLCUQwojcUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df47fd1f-293a-4815-daf0-1f4bd3aa9c75"
      },
      "source": [
        "print(batch_emb)\r\n",
        "print(batch_emb.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 6.9369e-02,  2.4249e-01, -2.2282e+00,  1.7414e-01, -8.2723e-01,\n",
            "           9.4986e-01, -1.6038e+00, -3.2543e+00],\n",
            "         [-3.8947e-01, -1.2184e+00,  5.2152e-01,  5.4078e-01,  3.2251e-01,\n",
            "          -8.6470e-01, -4.5789e-01,  1.7526e-01],\n",
            "         [ 1.8231e+00, -4.9956e-01,  4.6228e-01, -2.1515e-01, -8.4279e-01,\n",
            "           3.4500e-01, -1.8330e+00,  8.8227e-01],\n",
            "         [ 3.8412e-01,  3.1842e-01,  6.8199e-01,  4.0101e-01,  2.1617e+00,\n",
            "          -1.3141e+00,  1.1425e+00,  3.4231e-01],\n",
            "         [ 1.2836e+00, -3.9056e-01,  1.1230e+00,  3.9701e-01,  2.1289e-01,\n",
            "          -1.1325e+00,  6.1501e-01,  1.7247e-02],\n",
            "         [ 1.9910e-01,  6.9825e-01,  5.1294e-02, -3.8695e-01, -2.6612e-01,\n",
            "          -9.0221e-02, -1.1006e+00, -1.3427e+00],\n",
            "         [-5.4646e-01,  1.3041e+00, -2.2457e-01,  3.8844e-01,  2.3447e+00,\n",
            "           3.5552e-01, -4.0749e-01, -2.2848e+00],\n",
            "         [-3.8947e-01, -1.2184e+00,  5.2152e-01,  5.4078e-01,  3.2251e-01,\n",
            "          -8.6470e-01, -4.5789e-01,  1.7526e-01],\n",
            "         [ 3.4301e-01,  2.3752e-01, -2.2905e-01, -1.1945e+00,  2.4549e+00,\n",
            "           8.9956e-02,  7.7040e-01, -8.0499e-01],\n",
            "         [ 3.4301e-01,  2.3752e-01, -2.2905e-01, -1.1945e+00,  2.4549e+00,\n",
            "           8.9956e-02,  7.7040e-01, -8.0499e-01]],\n",
            "\n",
            "        [[-5.9270e-01, -3.5513e-02, -1.4861e+00,  1.2768e+00,  1.1901e-01,\n",
            "          -4.4125e-01,  2.0484e+00,  7.1255e-01],\n",
            "         [ 3.3016e-02,  1.0003e+00,  1.1048e+00,  1.1481e+00, -1.3341e-01,\n",
            "          -6.8369e-03, -9.9840e-01,  1.0967e+00],\n",
            "         [ 3.7040e-02, -3.9247e-01,  3.1573e-01,  3.5201e-01,  1.9133e+00,\n",
            "           6.3941e-01, -2.4918e-01, -9.5234e-02],\n",
            "         [-2.7736e-01, -5.8211e-01, -4.4534e-01, -6.0622e-01,  7.7277e-01,\n",
            "          -2.1711e-01,  1.5268e+00,  2.9537e+00],\n",
            "         [-4.7132e-01,  8.3641e-01, -7.8700e-01, -1.4010e+00, -4.1974e-01,\n",
            "           1.7555e+00,  7.1288e-02,  1.4044e+00],\n",
            "         [ 3.4301e-01,  2.3752e-01, -2.2905e-01, -1.1945e+00,  2.4549e+00,\n",
            "           8.9956e-02,  7.7040e-01, -8.0499e-01],\n",
            "         [ 3.4301e-01,  2.3752e-01, -2.2905e-01, -1.1945e+00,  2.4549e+00,\n",
            "           8.9956e-02,  7.7040e-01, -8.0499e-01],\n",
            "         [ 3.4301e-01,  2.3752e-01, -2.2905e-01, -1.1945e+00,  2.4549e+00,\n",
            "           8.9956e-02,  7.7040e-01, -8.0499e-01],\n",
            "         [ 3.4301e-01,  2.3752e-01, -2.2905e-01, -1.1945e+00,  2.4549e+00,\n",
            "           8.9956e-02,  7.7040e-01, -8.0499e-01],\n",
            "         [ 3.4301e-01,  2.3752e-01, -2.2905e-01, -1.1945e+00,  2.4549e+00,\n",
            "           8.9956e-02,  7.7040e-01, -8.0499e-01]],\n",
            "\n",
            "        [[ 1.9424e+00, -1.2545e+00, -3.6239e-01, -5.5196e-01,  9.8951e-01,\n",
            "          -1.2032e+00, -1.1738e+00,  8.1436e-01],\n",
            "         [ 3.4303e+00,  9.8337e-01, -1.1478e+00, -1.8494e-01,  4.9140e-04,\n",
            "          -3.7841e-01, -7.4728e-01,  5.0835e-01],\n",
            "         [ 6.1331e-01,  1.1327e+00,  9.2421e-01,  6.1838e-01, -1.8377e+00,\n",
            "           8.8987e-01, -1.4375e-01,  1.9488e+00],\n",
            "         [ 1.1536e+00, -2.6551e-01, -2.7954e+00,  1.4763e-01,  1.2165e+00,\n",
            "          -5.2983e-01, -6.2022e-01,  1.7067e+00],\n",
            "         [-1.0946e+00, -1.0811e+00, -1.8720e+00, -1.1547e-01,  7.7181e-02,\n",
            "          -4.4381e-01,  6.9362e-01, -1.0284e+00],\n",
            "         [-8.2028e-02,  4.0938e-01, -6.4292e-01,  5.7625e-01,  1.8332e+00,\n",
            "           1.8326e-01, -1.4423e+00,  5.3732e-01],\n",
            "         [-9.4960e-01, -4.0639e-01, -1.2575e-01,  3.4033e-01,  1.2424e+00,\n",
            "           1.2271e+00,  2.5546e+00,  1.9312e+00],\n",
            "         [ 3.9455e-01, -1.1796e+00, -1.6222e-01,  5.5053e-01, -1.4510e+00,\n",
            "           1.5664e-01, -1.3990e+00,  1.0321e+00],\n",
            "         [ 4.3689e-01,  1.7246e+00,  1.7909e+00, -1.8243e-01,  1.4167e-02,\n",
            "           8.4517e-01,  4.3631e-03, -1.4581e+00],\n",
            "         [-1.5464e-01,  5.9485e-01, -2.0119e-01,  1.8045e-01, -1.5245e+00,\n",
            "           1.5766e+00, -1.7907e+00,  1.1248e+00]],\n",
            "\n",
            "        [[ 2.6675e-02, -1.3359e+00,  6.0857e-01,  6.3635e-01, -1.0805e+00,\n",
            "          -2.9782e-01, -1.8198e+00,  1.6904e-01],\n",
            "         [ 1.0100e+00, -4.7266e-01,  8.6686e-01, -1.2013e+00,  8.7447e-01,\n",
            "           4.4628e-01,  1.3854e+00, -5.5321e-02],\n",
            "         [-2.1325e+00, -1.4109e+00, -1.1465e+00, -4.8086e-01,  1.7557e+00,\n",
            "          -2.9114e-01,  2.3455e+00,  9.5117e-01],\n",
            "         [ 1.8231e+00, -4.9956e-01,  4.6228e-01, -2.1515e-01, -8.4279e-01,\n",
            "           3.4500e-01, -1.8330e+00,  8.8227e-01],\n",
            "         [ 3.4301e-01,  2.3752e-01, -2.2905e-01, -1.1945e+00,  2.4549e+00,\n",
            "           8.9956e-02,  7.7040e-01, -8.0499e-01],\n",
            "         [ 3.4301e-01,  2.3752e-01, -2.2905e-01, -1.1945e+00,  2.4549e+00,\n",
            "           8.9956e-02,  7.7040e-01, -8.0499e-01],\n",
            "         [ 3.4301e-01,  2.3752e-01, -2.2905e-01, -1.1945e+00,  2.4549e+00,\n",
            "           8.9956e-02,  7.7040e-01, -8.0499e-01],\n",
            "         [ 3.4301e-01,  2.3752e-01, -2.2905e-01, -1.1945e+00,  2.4549e+00,\n",
            "           8.9956e-02,  7.7040e-01, -8.0499e-01],\n",
            "         [ 3.4301e-01,  2.3752e-01, -2.2905e-01, -1.1945e+00,  2.4549e+00,\n",
            "           8.9956e-02,  7.7040e-01, -8.0499e-01],\n",
            "         [ 3.4301e-01,  2.3752e-01, -2.2905e-01, -1.1945e+00,  2.4549e+00,\n",
            "           8.9956e-02,  7.7040e-01, -8.0499e-01]],\n",
            "\n",
            "        [[ 2.2616e-01,  2.0574e-01,  5.7538e-02, -6.6736e-01,  7.3683e-01,\n",
            "          -8.8170e-02,  7.3341e-01,  3.4783e-01],\n",
            "         [ 1.1536e+00, -2.6551e-01, -2.7954e+00,  1.4763e-01,  1.2165e+00,\n",
            "          -5.2983e-01, -6.2022e-01,  1.7067e+00],\n",
            "         [ 3.7040e-02, -3.9247e-01,  3.1573e-01,  3.5201e-01,  1.9133e+00,\n",
            "           6.3941e-01, -2.4918e-01, -9.5234e-02],\n",
            "         [ 2.2616e-01,  2.0574e-01,  5.7538e-02, -6.6736e-01,  7.3683e-01,\n",
            "          -8.8170e-02,  7.3341e-01,  3.4783e-01],\n",
            "         [ 1.7363e+00,  1.0095e+00, -9.2841e-01, -2.0401e+00, -8.8198e-02,\n",
            "           8.7740e-01, -3.6961e-01, -1.3459e+00],\n",
            "         [-1.4764e+00,  2.2721e-01, -9.8545e-01,  2.3885e-01, -9.8826e-01,\n",
            "           1.3872e+00, -2.4108e+00, -2.2071e-01],\n",
            "         [ 1.9424e+00, -1.2545e+00, -3.6239e-01, -5.5196e-01,  9.8951e-01,\n",
            "          -1.2032e+00, -1.1738e+00,  8.1436e-01],\n",
            "         [ 1.7363e+00,  1.0095e+00, -9.2841e-01, -2.0401e+00, -8.8198e-02,\n",
            "           8.7740e-01, -3.6961e-01, -1.3459e+00],\n",
            "         [-1.0427e+00, -6.7083e-01,  4.3858e-01,  1.5165e+00,  1.8250e-01,\n",
            "          -7.7785e-01, -9.1450e-02, -6.8606e-02],\n",
            "         [ 3.4301e-01,  2.3752e-01, -2.2905e-01, -1.1945e+00,  2.4549e+00,\n",
            "           8.9956e-02,  7.7040e-01, -8.0499e-01]]],\n",
            "       grad_fn=<EmbeddingBackward>)\n",
            "torch.Size([5, 10, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO3gxeyhpyF2"
      },
      "source": [
        "### **Mask 구축**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NDEQF64p5pN"
      },
      "source": [
        "`True`는 attention이 적용될 부분, `False`는 masking될 자리입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB0A4elupM2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "136aa53a-84a6-4269-a497-e5c249ee4212"
      },
      "source": [
        "padding_mask = (batch != pad_id).unsqueeze(1)  # (B, 1, L)\r\n",
        "\r\n",
        "print(padding_mask)\r\n",
        "print(padding_mask.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True, False, False]],\n",
            "\n",
            "        [[ True,  True,  True,  True,  True, False, False, False, False, False]],\n",
            "\n",
            "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
            "\n",
            "        [[ True,  True,  True,  True, False, False, False, False, False, False]],\n",
            "\n",
            "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True, False]]])\n",
            "torch.Size([5, 1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88cD54evrEo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04500a5c-ff07-46ff-f4c0-c1372a28b55b"
      },
      "source": [
        "nopeak_mask = torch.ones([1, max_len, max_len], dtype=torch.bool)  # (1, L, L)\r\n",
        "nopeak_mask = torch.tril(nopeak_mask)  # (1, L, L)\r\n",
        "\r\n",
        "print(nopeak_mask)\r\n",
        "print(nopeak_mask.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ True, False, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n",
            "torch.Size([1, 10, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMzB8_jarycy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "906023c1-5c5e-4396-c7eb-36378d348892"
      },
      "source": [
        "mask = padding_mask & nopeak_mask  # (B, L, L)\r\n",
        "\r\n",
        "print(mask)\r\n",
        "print(mask.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ True, False, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False]],\n",
            "\n",
            "        [[ True, False, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False]],\n",
            "\n",
            "        [[ True, False, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
            "\n",
            "        [[ True, False, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False]],\n",
            "\n",
            "        [[ True, False, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False]]])\n",
            "torch.Size([5, 10, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urXMBRnRgqvw"
      },
      "source": [
        "### **Linear transformation & 여러 head로 나누기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DWKDqgCgfMk"
      },
      "source": [
        "w_q = nn.Linear(d_model, d_model)\r\n",
        "w_k = nn.Linear(d_model, d_model)\r\n",
        "w_v = nn.Linear(d_model, d_model)\r\n",
        "\r\n",
        "w_0 = nn.Linear(d_model, d_model)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-vSL7PwnV6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "604d57e9-740c-4704-dc12-f0e35cd41c63"
      },
      "source": [
        "q = w_q(batch_emb)  # (B, L, d_model)\r\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\r\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\r\n",
        "\r\n",
        "batch_size = q.shape[0]\r\n",
        "d_k = d_model // num_heads\r\n",
        "\r\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "\r\n",
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "print(q.shape)\r\n",
        "print(k.shape)\r\n",
        "print(v.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 2, 10, 4])\n",
            "torch.Size([5, 2, 10, 4])\n",
            "torch.Size([5, 2, 10, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWrDA5_Sofad"
      },
      "source": [
        "### **Masking이 적용된 self-attention 구현**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqaQmVQdvMZB"
      },
      "source": [
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adlRCt6mvMy5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31879728-83f6-4a8d-ca94-de7269cefd42"
      },
      "source": [
        "masks = mask.unsqueeze(1)  # (B, 1, L, L)\r\n",
        "masked_attn_scores = attn_scores.masked_fill_(masks == False, -1 * inf)  # (B, num_heads, L, L)\r\n",
        "\r\n",
        "print(masked_attn_scores)\r\n",
        "print(masked_attn_scores.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[ 2.2115e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 7.7819e-01, -7.5931e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.4676e-01, -1.7861e-01, -2.0523e-01, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.1725e+00, -2.4820e-01,  2.1985e-01,  1.3675e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.5609e+00, -3.9705e-01,  1.0799e-01, -4.1059e-01, -1.7818e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 4.9092e-01, -2.1177e-01,  6.4167e-02, -2.5233e-01, -1.9928e-01,\n",
            "            5.8847e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.0201e+00, -4.2849e-01,  2.5911e-01, -5.5188e-02,  8.0276e-02,\n",
            "            1.7421e-01,  5.3618e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 7.7819e-01, -7.5931e-02, -7.0972e-02, -3.2703e-01, -1.9479e-01,\n",
            "            2.5011e-01,  3.7570e-01, -7.5931e-02, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.1714e-02, -1.3934e-01,  1.4613e-01,  3.8142e-01,  2.9507e-01,\n",
            "           -5.9939e-02,  3.9362e-01, -1.3934e-01, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.1714e-02, -1.3934e-01,  1.4613e-01,  3.8142e-01,  2.9507e-01,\n",
            "           -5.9939e-02,  3.9362e-01, -1.3934e-01, -1.0000e+12, -1.0000e+12]],\n",
            "\n",
            "         [[ 1.3704e+00, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.2417e-01, -2.3588e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.5120e-01, -1.5691e-01,  2.5023e-01, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.0995e+00, -2.6603e-01, -7.7633e-01,  3.1159e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-8.1675e-01, -3.1264e-01, -3.8131e-01,  1.6580e-01, -1.3019e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 4.5357e-01,  1.0199e-01,  5.1775e-01, -2.8302e-01,  2.4896e-01,\n",
            "            1.8453e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 3.2142e-01,  4.5892e-02,  2.2285e-01, -1.4951e-01,  5.6234e-02,\n",
            "            1.0238e-01, -1.6501e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.2417e-01, -2.3588e-02, -1.4932e-01,  6.4847e-02, -4.4100e-02,\n",
            "           -1.0289e-01,  3.8091e-02, -2.3588e-02, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.3463e-01,  2.2165e-01, -4.2997e-01,  1.6654e-01, -1.9073e-01,\n",
            "           -1.5306e-01,  2.1996e-01,  2.2165e-01, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.3463e-01,  2.2165e-01, -4.2997e-01,  1.6654e-01, -1.9073e-01,\n",
            "           -1.5306e-01,  2.1996e-01,  2.2165e-01, -1.0000e+12, -1.0000e+12]]],\n",
            "\n",
            "\n",
            "        [[[-1.2820e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.7061e-01,  4.2384e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 9.1472e-02, -6.2281e-02,  1.2090e-01, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-6.7406e-01, -1.6373e-01,  2.5847e-01,  2.9393e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-4.6630e-01, -3.4532e-01, -6.4394e-02,  1.3544e+00, -2.5407e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.0877e-01,  8.1278e-02,  4.4652e-01,  8.4994e-02,  4.8764e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.0877e-01,  8.1278e-02,  4.4652e-01,  8.4994e-02,  4.8764e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.0877e-01,  8.1278e-02,  4.4652e-01,  8.4994e-02,  4.8764e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.0877e-01,  8.1278e-02,  4.4652e-01,  8.4994e-02,  4.8764e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.0877e-01,  8.1278e-02,  4.4652e-01,  8.4994e-02,  4.8764e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]],\n",
            "\n",
            "         [[-1.9982e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.4604e-01,  4.5123e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-5.6721e-02, -1.8272e-01,  1.2132e-01, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.9696e-01, -6.5099e-01,  5.2331e-01,  4.3843e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 8.5207e-02, -4.0604e-01,  4.8041e-02, -4.7224e-05, -4.2595e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-9.5049e-02, -4.9116e-01,  2.4321e-01,  1.9050e-01, -1.5791e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-9.5049e-02, -4.9116e-01,  2.4321e-01,  1.9050e-01, -1.5791e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-9.5049e-02, -4.9116e-01,  2.4321e-01,  1.9050e-01, -1.5791e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-9.5049e-02, -4.9116e-01,  2.4321e-01,  1.9050e-01, -1.5791e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-9.5049e-02, -4.9116e-01,  2.4321e-01,  1.9050e-01, -1.5791e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]]],\n",
            "\n",
            "\n",
            "        [[[-1.6603e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.1658e-01,  2.9679e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 4.8575e-02, -2.1074e-01, -1.0219e-01, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 3.0498e-01, -8.3332e-01, -5.6739e-01, -4.8721e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 9.1822e-02, -4.2354e-01, -2.2356e-01, -5.4919e-01, -1.1229e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 7.2291e-03, -8.6948e-02, -1.1976e-01,  5.4367e-02,  2.1238e-02,\n",
            "            4.4285e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 4.0493e-01, -2.7989e-01, -4.2758e-01, -2.2247e-01, -2.5616e-01,\n",
            "            1.0971e-02,  2.1953e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-8.4911e-02, -8.4604e-01, -5.5221e-01,  2.1339e-02,  2.5563e-01,\n",
            "           -2.8945e-02, -6.4712e-01,  8.0652e-02, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.8412e-01,  1.2238e+00,  7.5358e-01,  9.7097e-01,  1.5048e-01,\n",
            "            3.5904e-01,  4.9533e-01,  1.9098e-01,  2.1979e-01, -1.0000e+12],\n",
            "          [ 3.0494e-01, -7.8401e-01, -5.1067e-01,  4.4629e-01,  2.0121e-01,\n",
            "           -1.4228e-02, -2.1583e-01,  8.6328e-02, -1.2843e+00, -9.9483e-02]],\n",
            "\n",
            "         [[-4.0631e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-7.6107e-01, -1.0530e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-5.4738e-01, -8.6231e-02,  8.6120e-01, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-3.4208e-01, -8.2862e-01, -2.7383e-01, -9.6575e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 5.6074e-01, -3.7484e-01, -8.7638e-01,  8.3694e-02,  6.0277e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.9157e-01, -1.0432e-01,  1.7945e-01, -2.9022e-01, -2.5390e-01,\n",
            "           -5.6933e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-6.6947e-02, -1.5828e+00, -1.4835e+00, -2.1958e-01,  5.5403e-01,\n",
            "            1.1071e-01,  6.8915e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-6.4166e-02,  1.8748e-01,  4.3304e-01, -2.6512e-01, -2.5132e-01,\n",
            "           -1.2103e-01, -1.3649e-01,  2.0742e-01, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.0150e-01,  4.7607e-01,  7.2619e-01,  2.6468e-01,  1.8779e-02,\n",
            "           -1.9963e-01,  6.2734e-02,  4.7956e-01, -1.0855e-01, -1.0000e+12],\n",
            "          [ 1.4428e-01,  5.1783e-01,  6.6359e-01,  3.3727e-02,  2.1039e-02,\n",
            "           -2.2385e-01, -3.5783e-01,  7.0711e-01,  6.3016e-02,  4.9230e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.6422e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.8920e-01,  1.6455e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.3672e-01,  8.4397e-01,  2.4756e-01, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-3.1881e-02, -7.7507e-01,  2.3465e-01, -2.0523e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.7918e-01,  5.1449e-01, -6.3932e-02,  1.4613e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.7918e-01,  5.1449e-01, -6.3932e-02,  1.4613e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.7918e-01,  5.1449e-01, -6.3932e-02,  1.4613e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.7918e-01,  5.1449e-01, -6.3932e-02,  1.4613e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.7918e-01,  5.1449e-01, -6.3932e-02,  1.4613e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.7918e-01,  5.1449e-01, -6.3932e-02,  1.4613e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]],\n",
            "\n",
            "         [[ 2.3363e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.2196e-01,  1.2912e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-8.0690e-02,  2.5843e-01,  2.0027e+00, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 4.0100e-02,  7.1537e-03, -6.6382e-01,  2.5023e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-9.9429e-02,  1.0992e-01,  8.7341e-01, -4.2997e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-9.9429e-02,  1.0992e-01,  8.7341e-01, -4.2997e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-9.9429e-02,  1.0992e-01,  8.7341e-01, -4.2997e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-9.9429e-02,  1.0992e-01,  8.7341e-01, -4.2997e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-9.9429e-02,  1.0992e-01,  8.7341e-01, -4.2997e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-9.9429e-02,  1.0992e-01,  8.7341e-01, -4.2997e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0911e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.9578e-02, -4.8721e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-7.5508e-02,  3.5339e-01,  1.2090e-01, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.0911e-01, -2.4339e-02,  2.1787e-01,  1.0911e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.9288e-01,  8.1591e-01, -3.8195e-03,  1.9288e-01,  4.4770e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-7.6899e-02,  2.1299e-01, -5.0226e-01, -7.6899e-02, -6.5874e-01,\n",
            "            1.0138e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.0735e-01, -9.0008e-02, -2.7840e-02, -2.0735e-01, -4.4121e-02,\n",
            "            3.4349e-01, -1.6603e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.9288e-01,  8.1591e-01, -3.8195e-03,  1.9288e-01,  4.4770e-02,\n",
            "           -1.5140e-01,  3.4112e-01,  4.4770e-02, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.4299e-01, -3.6552e-01, -3.4527e-02, -2.4299e-01,  2.2379e-01,\n",
            "            5.0900e-01, -4.7885e-01,  2.2379e-01,  1.4759e-01, -1.0000e+12],\n",
            "          [ 2.0454e-01,  2.0907e-01,  4.4652e-01,  2.0454e-01,  3.5095e-01,\n",
            "           -3.8734e-01,  1.3896e-01,  3.5095e-01, -1.1450e-01, -1.0000e+12]],\n",
            "\n",
            "         [[ 1.8445e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 5.6084e-02, -9.6575e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 5.7442e-02, -1.5831e-02,  1.2132e-01, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.8445e-02, -1.8000e-01,  1.6826e-01,  1.8445e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.1955e-01,  1.1339e-01, -8.9309e-02, -1.1955e-01,  1.5551e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.2553e-01,  4.5321e-01, -3.5459e-01, -2.2553e-01,  2.4627e-01,\n",
            "            5.3931e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 8.0244e-02, -6.6030e-01,  2.4559e-01,  8.0244e-02, -1.8719e-01,\n",
            "           -3.6173e-01, -4.0631e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.1955e-01,  1.1339e-01, -8.9309e-02, -1.1955e-01,  1.5551e-02,\n",
            "            2.3446e-01,  3.9445e-01,  1.5551e-02, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.1557e-03, -1.8615e-01, -4.7246e-03, -1.1557e-03, -1.3744e-01,\n",
            "           -8.8055e-02, -2.3051e-01, -1.3744e-01, -2.0096e-02, -1.0000e+12],\n",
            "          [ 5.9903e-02,  4.3955e-02,  2.4321e-01,  5.9903e-02, -2.2673e-01,\n",
            "           -9.9508e-02,  1.5364e-01, -2.2673e-01,  7.6437e-02, -1.0000e+12]]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "torch.Size([5, 2, 10, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EqMuVJFwHhI"
      },
      "source": [
        "`-1* inf`로 masking된 부분은 softmax 후 0이 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVNze4elv4Uf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb243dd-b557-473a-89e4-56f826fa16d5"
      },
      "source": [
        "attn_dists = F.softmax(masked_attn_scores, dim=-1)  # (B, num_heads, L, L)\r\n",
        "\r\n",
        "print(attn_dists)\r\n",
        "print(attn_dists.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.7014, 0.2986, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.4367, 0.2854, 0.2779, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.5045, 0.1219, 0.1946, 0.1791, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.5917, 0.0835, 0.1384, 0.0824, 0.1040, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2650, 0.1312, 0.1729, 0.1260, 0.1329, 0.1720, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2874, 0.0675, 0.1343, 0.0981, 0.1123, 0.1233, 0.1771, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2355, 0.1002, 0.1007, 0.0780, 0.0890, 0.1389, 0.1575, 0.1002,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1117, 0.0951, 0.1265, 0.1600, 0.1468, 0.1029, 0.1620, 0.0951,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1117, 0.0951, 0.1265, 0.1600, 0.1468, 0.1029, 0.1620, 0.0951,\n",
            "           0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.4500, 0.5500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2867, 0.2850, 0.4283, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1139, 0.2620, 0.1573, 0.4668, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1129, 0.1869, 0.1745, 0.3015, 0.2243, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2071, 0.1457, 0.2209, 0.0992, 0.1688, 0.1583, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1827, 0.1387, 0.1655, 0.1141, 0.1401, 0.1467, 0.1123, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1055, 0.1289, 0.1136, 0.1408, 0.1263, 0.1190, 0.1371, 0.1289,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1075, 0.1535, 0.0800, 0.1452, 0.1016, 0.1055, 0.1532, 0.1535,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1075, 0.1535, 0.0800, 0.1452, 0.1016, 0.1055, 0.1532, 0.1535,\n",
            "           0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.5320, 0.4680, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3463, 0.2970, 0.3567, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1276, 0.2125, 0.3241, 0.3358, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.0881, 0.0994, 0.1316, 0.5440, 0.1369, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2047, 0.1802, 0.2597, 0.1809, 0.1745, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2047, 0.1802, 0.2597, 0.1809, 0.1745, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2047, 0.1802, 0.2597, 0.1809, 0.1745, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2047, 0.1802, 0.2597, 0.1809, 0.1745, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2047, 0.1802, 0.2597, 0.1809, 0.1745, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.4489, 0.5511, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3250, 0.2866, 0.3884, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1793, 0.1139, 0.3684, 0.3384, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2443, 0.1495, 0.2354, 0.2243, 0.1465, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1871, 0.1259, 0.2624, 0.2489, 0.1757, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1871, 0.1259, 0.2624, 0.2489, 0.1757, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1871, 0.1259, 0.2624, 0.2489, 0.1757, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1871, 0.1259, 0.2624, 0.2489, 0.1757, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1871, 0.1259, 0.2624, 0.2489, 0.1757, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3981, 0.6019, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3800, 0.2932, 0.3268, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.4564, 0.1462, 0.1907, 0.2067, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2726, 0.1628, 0.1988, 0.1436, 0.2222, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1698, 0.1545, 0.1495, 0.1779, 0.1721, 0.1762, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2225, 0.1122, 0.0968, 0.1188, 0.1149, 0.1500, 0.1848, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1348, 0.0630, 0.0845, 0.1499, 0.1895, 0.1425, 0.0768, 0.1591,\n",
            "           0.0000, 0.0000],\n",
            "          [0.0482, 0.2178, 0.1361, 0.1692, 0.0745, 0.0917, 0.1051, 0.0775,\n",
            "           0.0798, 0.0000],\n",
            "          [0.1465, 0.0493, 0.0648, 0.1687, 0.1320, 0.1064, 0.0870, 0.1177,\n",
            "           0.0299, 0.0977]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3417, 0.6583, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1498, 0.2376, 0.6127, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3104, 0.1908, 0.3324, 0.1664, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3036, 0.1191, 0.0721, 0.1884, 0.3167, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1531, 0.1671, 0.2219, 0.1388, 0.1439, 0.1752, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1332, 0.0293, 0.0323, 0.1144, 0.2479, 0.1591, 0.2838, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1141, 0.1467, 0.1876, 0.0933, 0.0946, 0.1078, 0.1061, 0.1497,\n",
            "           0.0000, 0.0000],\n",
            "          [0.0800, 0.1425, 0.1830, 0.1153, 0.0902, 0.0725, 0.0942, 0.1430,\n",
            "           0.0794, 0.0000],\n",
            "          [0.0885, 0.1285, 0.1487, 0.0792, 0.0782, 0.0612, 0.0535, 0.1553,\n",
            "           0.0816, 0.1253]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3885, 0.6115, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2412, 0.4893, 0.2695, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2761, 0.1313, 0.3604, 0.2322, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1672, 0.3697, 0.2073, 0.2558, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1672, 0.3697, 0.2073, 0.2558, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1672, 0.3697, 0.2073, 0.2558, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1672, 0.3697, 0.2073, 0.2558, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1672, 0.3697, 0.2073, 0.2558, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1672, 0.3697, 0.2073, 0.2558, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.4131, 0.5869, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.0958, 0.1345, 0.7696, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2706, 0.2618, 0.1338, 0.3338, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1787, 0.2203, 0.4727, 0.1284, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1787, 0.2203, 0.4727, 0.1284, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1787, 0.2203, 0.4727, 0.1284, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1787, 0.2203, 0.4727, 0.1284, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1787, 0.2203, 0.4727, 0.1284, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1787, 0.2203, 0.4727, 0.1284, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.6264, 0.3736, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2665, 0.4092, 0.3243, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2506, 0.2193, 0.2794, 0.2506, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1802, 0.3361, 0.1481, 0.1802, 0.1554, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1741, 0.2326, 0.1138, 0.1741, 0.0973, 0.2081, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1208, 0.1359, 0.1446, 0.1208, 0.1423, 0.2096, 0.1259, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1208, 0.2252, 0.0992, 0.1208, 0.1042, 0.0856, 0.1401, 0.1042,\n",
            "           0.0000, 0.0000],\n",
            "          [0.0855, 0.0756, 0.1053, 0.0855, 0.1364, 0.1814, 0.0675, 0.1364,\n",
            "           0.1264, 0.0000],\n",
            "          [0.1135, 0.1140, 0.1446, 0.1135, 0.1314, 0.0628, 0.1063, 0.1314,\n",
            "           0.0825, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.7353, 0.2647, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.3339, 0.3103, 0.3559, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.2512, 0.2059, 0.2917, 0.2512, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1839, 0.2321, 0.1895, 0.1839, 0.2105, 0.0000, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1163, 0.2292, 0.1022, 0.1163, 0.1863, 0.2498, 0.0000, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1761, 0.0840, 0.2077, 0.1761, 0.1348, 0.1132, 0.1082, 0.0000,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1033, 0.1304, 0.1065, 0.1033, 0.1183, 0.1472, 0.1727, 0.1183,\n",
            "           0.0000, 0.0000],\n",
            "          [0.1210, 0.1005, 0.1205, 0.1210, 0.1056, 0.1109, 0.0962, 0.1056,\n",
            "           0.1187, 0.0000],\n",
            "          [0.1155, 0.1137, 0.1388, 0.1155, 0.0868, 0.0985, 0.1269, 0.0868,\n",
            "           0.1175, 0.0000]]]], grad_fn=<SoftmaxBackward>)\n",
            "torch.Size([5, 2, 10, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBwm34bswV7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "428fec78-c3fb-4da3-f7ce-e346678980b7"
      },
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 2, 10, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2Xab7WKzTEU"
      },
      "source": [
        "### **전체 코드**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlF7R6DIzVWc"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(MultiheadAttention, self).__init__()\r\n",
        "\r\n",
        "    # Q, K, V learnable matrices\r\n",
        "    self.w_q = nn.Linear(d_model, d_model)\r\n",
        "    self.w_k = nn.Linear(d_model, d_model)\r\n",
        "    self.w_v = nn.Linear(d_model, d_model)\r\n",
        "\r\n",
        "    # Linear transformation for concatenated outputs\r\n",
        "    self.w_0 = nn.Linear(d_model, d_model)\r\n",
        "\r\n",
        "  def forward(self, q, k, v, mask=None):\r\n",
        "    batch_size = q.shape[0]\r\n",
        "\r\n",
        "    q = self.w_q(q)  # (B, L, d_model)\r\n",
        "    k = self.w_k(k)  # (B, L, d_model)\r\n",
        "    v = self.w_v(v)  # (B, L, d_model)\r\n",
        "\r\n",
        "    q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "    k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "    v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "\r\n",
        "    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "    attn_values = self.self_attention(q, k, v, mask=mask)  # (B, num_heads, L, d_k)\r\n",
        "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\r\n",
        "\r\n",
        "    return self.w_0(attn_values)\r\n",
        "\r\n",
        "  def self_attention(self, q, k, v, mask=None):\r\n",
        "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\r\n",
        "\r\n",
        "    if mask is not None:\r\n",
        "      mask = mask.unsqueeze(1)  # (B, 1, L, L) or  (B, 1, 1, L)\r\n",
        "      attn_scores = attn_scores.masked_fill_(mask == False, -1*inf)\r\n",
        "\r\n",
        "    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\r\n",
        "\r\n",
        "    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "    return attn_values"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYLuu_9alQxT"
      },
      "source": [
        "multihead_attn = MultiheadAttention()\r\n",
        "\r\n",
        "outputs = multihead_attn(batch_emb, batch_emb, batch_emb, mask=mask)  # (B, L, d_model)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMiXlYjSlTfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b5d5abd-55d2-46cf-93f2-17f132c54f43"
      },
      "source": [
        "print(outputs)\r\n",
        "print(outputs.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.1829,  0.1320,  0.2332, -0.4308,  0.5022, -0.2316,  0.1451,\n",
            "          -0.5199],\n",
            "         [-0.2285,  0.2218,  0.3632, -0.0978,  0.3831, -0.3067, -0.2191,\n",
            "          -0.3566],\n",
            "         [ 0.0339,  0.0832,  0.5525,  0.1662,  0.4973, -0.7794, -0.2804,\n",
            "          -0.4799],\n",
            "         [-0.1748,  0.4617,  0.4909,  0.3265,  0.2605, -0.4502, -0.4687,\n",
            "          -0.3909],\n",
            "         [-0.0022,  0.3678,  0.5065,  0.4509,  0.1854, -0.5408, -0.5827,\n",
            "          -0.2428],\n",
            "         [-0.0030,  0.3144,  0.5457,  0.3982,  0.2863, -0.6301, -0.5245,\n",
            "          -0.3172],\n",
            "         [-0.0642,  0.3934,  0.5348,  0.4335,  0.2515, -0.5855, -0.5460,\n",
            "          -0.3237],\n",
            "         [-0.1754,  0.4056,  0.4611,  0.2961,  0.2640, -0.4733, -0.4301,\n",
            "          -0.3666],\n",
            "         [-0.1185,  0.3612,  0.5340,  0.3256,  0.3284, -0.5856, -0.4802,\n",
            "          -0.3799],\n",
            "         [-0.1185,  0.3612,  0.5340,  0.3256,  0.3284, -0.5856, -0.4802,\n",
            "          -0.3799]],\n",
            "\n",
            "        [[-0.0469,  0.5787,  0.5715, -0.0500,  0.1439, -0.0070, -0.9498,\n",
            "           0.1512],\n",
            "         [-0.1451,  0.5295,  0.5663,  0.2912,  0.0850, -0.3606, -0.8773,\n",
            "           0.0057],\n",
            "         [-0.1406,  0.5323,  0.4287,  0.3377,  0.0058, -0.2807, -0.7050,\n",
            "          -0.0808],\n",
            "         [-0.0467,  0.5520,  0.3684,  0.3757, -0.0684, -0.2108, -0.6554,\n",
            "          -0.1225],\n",
            "         [ 0.1221,  0.5027,  0.4289,  0.2749,  0.0316, -0.2731, -0.5750,\n",
            "          -0.1644],\n",
            "         [ 0.0546,  0.5162,  0.4366,  0.3134, -0.0016, -0.2634, -0.6528,\n",
            "          -0.1089],\n",
            "         [ 0.0546,  0.5162,  0.4366,  0.3134, -0.0016, -0.2634, -0.6528,\n",
            "          -0.1089],\n",
            "         [ 0.0546,  0.5162,  0.4366,  0.3134, -0.0016, -0.2634, -0.6528,\n",
            "          -0.1089],\n",
            "         [ 0.0546,  0.5162,  0.4366,  0.3134, -0.0016, -0.2634, -0.6528,\n",
            "          -0.1089],\n",
            "         [ 0.0546,  0.5162,  0.4366,  0.3134, -0.0016, -0.2634, -0.6528,\n",
            "          -0.1089]],\n",
            "\n",
            "        [[-0.1539,  0.3456,  0.4631,  0.6440,  0.2128, -0.6274, -0.2523,\n",
            "          -0.6643],\n",
            "         [ 0.0612,  0.3799,  0.5421,  0.6227,  0.2814, -0.7832, -0.2551,\n",
            "          -0.7205],\n",
            "         [ 0.2435,  0.2841,  0.5559,  0.5113,  0.2492, -0.7860, -0.4596,\n",
            "          -0.4918],\n",
            "         [ 0.0358,  0.3343,  0.5028,  0.5533,  0.1157, -0.6067, -0.5094,\n",
            "          -0.3862],\n",
            "         [-0.0471,  0.4129,  0.5789,  0.4639,  0.2271, -0.6009, -0.4355,\n",
            "          -0.4436],\n",
            "         [-0.0683,  0.4105,  0.4693,  0.3726,  0.1211, -0.4519, -0.4916,\n",
            "          -0.3121],\n",
            "         [-0.1831,  0.4510,  0.3208,  0.3644, -0.0050, -0.2454, -0.4060,\n",
            "          -0.3304],\n",
            "         [-0.0405,  0.3477,  0.4502,  0.4255,  0.0957, -0.4681, -0.5049,\n",
            "          -0.2969],\n",
            "         [ 0.1840,  0.4407,  0.5163,  0.3562,  0.1559, -0.5485, -0.5344,\n",
            "          -0.2834],\n",
            "         [ 0.0582,  0.3338,  0.4951,  0.3319,  0.1751, -0.5315, -0.4965,\n",
            "          -0.2797]],\n",
            "\n",
            "        [[-0.0889,  0.0019,  0.5664,  0.1450,  0.4600, -0.7090, -0.4781,\n",
            "          -0.2718],\n",
            "         [ 0.3588,  0.2095,  0.5277,  0.2733,  0.3607, -0.6425, -0.4251,\n",
            "          -0.3574],\n",
            "         [ 0.0177,  0.3763,  0.4770,  0.1726,  0.2089, -0.1989, -0.6288,\n",
            "          -0.1037],\n",
            "         [ 0.2766,  0.1813,  0.5417,  0.4079,  0.2629, -0.6557, -0.5038,\n",
            "          -0.2670],\n",
            "         [ 0.1953,  0.2435,  0.5220,  0.2936,  0.2922, -0.5272, -0.5045,\n",
            "          -0.2724],\n",
            "         [ 0.1953,  0.2435,  0.5220,  0.2936,  0.2922, -0.5272, -0.5045,\n",
            "          -0.2724],\n",
            "         [ 0.1953,  0.2435,  0.5220,  0.2936,  0.2922, -0.5272, -0.5045,\n",
            "          -0.2724],\n",
            "         [ 0.1953,  0.2435,  0.5220,  0.2936,  0.2922, -0.5272, -0.5045,\n",
            "          -0.2724],\n",
            "         [ 0.1953,  0.2435,  0.5220,  0.2936,  0.2922, -0.5272, -0.5045,\n",
            "          -0.2724],\n",
            "         [ 0.1953,  0.2435,  0.5220,  0.2936,  0.2922, -0.5272, -0.5045,\n",
            "          -0.2724]],\n",
            "\n",
            "        [[ 0.1338,  0.5339,  0.5344,  0.3894,  0.1991, -0.4401, -0.5521,\n",
            "          -0.3245],\n",
            "         [-0.2456,  0.5525,  0.3177,  0.4233, -0.0823, -0.1480, -0.4176,\n",
            "          -0.3322],\n",
            "         [-0.1782,  0.5641,  0.2757,  0.4399, -0.0989, -0.1708, -0.3848,\n",
            "          -0.3228],\n",
            "         [-0.0963,  0.5368,  0.3431,  0.4287, -0.0120, -0.2446, -0.4392,\n",
            "          -0.3144],\n",
            "         [-0.0223,  0.5777,  0.3550,  0.3943,  0.0259, -0.3139, -0.2966,\n",
            "          -0.4550],\n",
            "         [-0.2213,  0.3630,  0.4128,  0.1999,  0.2201, -0.4216, -0.3011,\n",
            "          -0.4026],\n",
            "         [-0.0715,  0.4253,  0.3400,  0.3442,  0.0946, -0.3454, -0.2730,\n",
            "          -0.4397],\n",
            "         [-0.0288,  0.4539,  0.3977,  0.3729,  0.1405, -0.4479, -0.2438,\n",
            "          -0.5011],\n",
            "         [-0.0439,  0.4098,  0.3829,  0.3307,  0.1364, -0.3840, -0.3371,\n",
            "          -0.3837],\n",
            "         [-0.0595,  0.4244,  0.3996,  0.3095,  0.1674, -0.3987, -0.3200,\n",
            "          -0.4228]]], grad_fn=<AddBackward0>)\n",
            "torch.Size([5, 10, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1g99JEEFwFv"
      },
      "source": [
        "### **Encoder-Decoder attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2PRoF4fF4ah"
      },
      "source": [
        "Query, key, value만 달라질 뿐 구현은 동일합니다.  \r\n",
        "Decoder에 들어갈 batch만 별도 구현하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p26ra2BsGEdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf17e5ad-aa92-465f-be1a-ee9ab02810d8"
      },
      "source": [
        "trg_data = [\r\n",
        "  [33, 11, 49, 10],\r\n",
        "  [88, 34, 5, 29, 99, 45, 11, 25],\r\n",
        "  [67, 25, 15, 90, 54, 4, 92, 10, 46, 20, 88 ,19],\r\n",
        "  [16, 58, 91, 47, 12, 5, 8],\r\n",
        "  [71, 63, 62, 7, 9, 11, 55, 91, 32, 48]\r\n",
        "]\r\n",
        "\r\n",
        "trg_data, trg_max_len = padding(trg_data)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 17260.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Maximum sequence length: 12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYysB4EKHKGI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac51cec4-5ea8-446e-e88d-e71682b2eb19"
      },
      "source": [
        "# S_L: source maximum sequence length, T_L: target maximum sequence length\r\n",
        "src_batch = batch  # (B, S_L)\r\n",
        "trg_batch = torch.LongTensor(trg_data)  # (B, T_L)\r\n",
        "\r\n",
        "print(src_batch.shape)\r\n",
        "print(trg_batch.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 10])\n",
            "torch.Size([5, 12])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AieDxWYIHXKc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f0c8dc2-cba0-455a-8e0c-2c05b4dd4c09"
      },
      "source": [
        "src_emb = embedding(src_batch)  # (B, S_L, d_w)\r\n",
        "trg_emb = embedding(trg_batch)  # (B, T_L, d_w)\r\n",
        "\r\n",
        "print(src_emb.shape)\r\n",
        "print(trg_emb.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 10, 8])\n",
            "torch.Size([5, 12, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxCjmPurH2b7"
      },
      "source": [
        "`src_emb`를 encoder에서 나온 결과, 그리고 `trg_emb`를 masked multi-head attention 후 결과로 가정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUhY-z8JHeUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97be2b09-f540-4d06-bfcc-5fc563b92ccc"
      },
      "source": [
        "q = w_q(trg_emb)  # (B, T_L, d_model)\r\n",
        "k = w_k(src_emb)  # (B, S_L, d_model)\r\n",
        "v = w_v(src_emb)  # (B, S_L, d_model)\r\n",
        "\r\n",
        "batch_size = q.shape[0]\r\n",
        "d_k = d_model // num_heads\r\n",
        "\r\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, T_L, num_heads, d_k)\r\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, S_L, num_heads, d_k)\r\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, S_L, num_heads, d_k)\r\n",
        "\r\n",
        "q = q.transpose(1, 2)  # (B, num_heads, T_L, d_k)\r\n",
        "k = k.transpose(1, 2)  # (B, num_heads, S_L, d_k)\r\n",
        "v = v.transpose(1, 2)  # (B, num_heads, S_L, d_k)\r\n",
        "\r\n",
        "print(q.shape)\r\n",
        "print(k.shape)\r\n",
        "print(v.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 2, 12, 4])\n",
            "torch.Size([5, 2, 10, 4])\n",
            "torch.Size([5, 2, 10, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeqjkVqkIdxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21faacd1-28e8-4c83-92b8-a71e80bd8c7f"
      },
      "source": [
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, T_L, S_L)\r\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, T_L, S_L)\r\n",
        "\r\n",
        "print(attn_dists.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 2, 12, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQv4IINbItS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a17ee1-50b9-4899-d86c-7995482ef03f"
      },
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, T_L, d_k)\r\n",
        "\r\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 2, 12, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLCHeCbtJDy9"
      },
      "source": [
        "Masked multi-head attention 후 나온 결과와 동일한 shape를 가지며 이후 layer에서 전체 연산도 동일하게 진행됩니다."
      ]
    }
  ]
}